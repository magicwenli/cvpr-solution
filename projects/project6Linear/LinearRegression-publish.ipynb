{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "bad magic number in 'helperP': b'B\\r\\r\\n'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-1c3efbddf7c4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mhelperP\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: bad magic number in 'helperP': b'B\\r\\r\\n'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from helperP import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'DATASET/'\n",
    "\n",
    "age_train, features_train = prepare_data('train', base_dir)\n",
    "age_val, features_val = prepare_data('val', base_dir)\n",
    "_, features_test = prepare_data('test', base_dir)\n",
    "show_data(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Closed Form Solution\n",
    "```\n",
    "Arguments:\n",
    "    age          -- numpy array, shape (n, )\n",
    "    features     -- numpy array, shape (n, 2048)\n",
    "Returns:\n",
    "    weights      -- numpy array, (2048, )\n",
    "    bias         -- numpy array, (1, )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form_solution(age, features):\n",
    "    # Preprocess\n",
    "    H = features\n",
    "    ones = np.ones(len(H))\n",
    "    H = np.column_stack((ones,H))  # 按列合并\n",
    "    Y = age\n",
    "\n",
    "    # n = features.shape[0]\n",
    "    \n",
    "    # Define parameter weights\n",
    "    \n",
    "    ##########################################################################\n",
    "    # TODO: YOUR CODE HERE\n",
    "    ########################################################################## \n",
    "    # calculate the closed form solution\n",
    "    weights = None\n",
    "\n",
    "    # EW+FB=G   ->  | E F | |W| = |G|\n",
    "    # HW+IB=J       | H I | |b|   |J|\n",
    "\n",
    "    weights = np.linalg.inv(H.T.dot(H)).dot(H.T).dot(Y)\n",
    "    \n",
    "    # E = np.sum(features*features,axis=1)\n",
    "    # F = np.sum(features,axis=1).T\n",
    "    # G = np.sum(np.multiply(features, Y[:, np.newaxis]),axis=1).T\n",
    "    # H = np.sum(features,axis=1)\n",
    "    # I = n\n",
    "    # J = np.sum(Y)\n",
    "    \n",
    "\n",
    "    # print(E)\n",
    "    # a=np.concatenate([E,F],axis=1)\n",
    "    # b=np.concatenate([H,I],axis=1)\n",
    "    # K=np.concatenate(a,b,axis=1)\n",
    "    # L=np.concatenate([G,J])\n",
    "\n",
    "    # weights=np.linalg.solve(K,L)\n",
    "\n",
    "    # separate the weights and bias\n",
    "    bias    = weights[0]\n",
    "    weights    = weights[1:]\n",
    "    \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = closed_form_solution(age_train, features_train)\n",
    "loss, pred = evaluate(w, b, age_val, features_val)\n",
    "print(\"Your validate loss is:\", round(loss, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Generate results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = test(w, b, features_test, 'cfs.txt')\n",
    "print(\"Test results has saved to cfs.txt\")\n",
    "print(prediction[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Gradient descent\n",
    "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.\n",
    "\n",
    "```\n",
    "Arguments:\n",
    "    age          -- numpy array, label, (n, )\n",
    "    feature      -- numpy array, features, (n, 2048)\n",
    "Return:\n",
    "    weights      -- numpy array, (2048, )\n",
    "    bias         -- numpy array, (1, )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(age, feature):\n",
    "    assert len(age) == len(feature)\n",
    "\n",
    "    # Init weights and bias\n",
    "    weights = np.random.randn(2048, 1)\n",
    "    bias = np.random.randn(1, 1)\n",
    "    m,n = feature.shape\n",
    "    # Learning rate\n",
    "    lr = 10e-3\n",
    "\n",
    "    # control the times of processing\n",
    "    for e in range(epoch):\n",
    "        ##########################################################################\n",
    "        # TODO: YOUR CODE HERE\n",
    "        ##########################################################################\n",
    "        l_w = np.zeros((2048, 1), dtype=float)\n",
    "        l_b = 0\n",
    "        \n",
    "        for i in range(m):\n",
    "            tmp = feature[i, :].reshape(-1, 1)\n",
    "            l_w += (np.dot(feature[i, :], weights) + bias - float(age[i])) * tmp\n",
    "        l_w /= m\n",
    "        for i in range(m):\n",
    "            l_b += np.dot(feature[i, :], weights) + bias - float(age[i])\n",
    "        l_b /= m\n",
    "\n",
    "        weights -= lr * l_w\n",
    "        bias -= lr * l_b\n",
    "        if momentum:\n",
    "            pass  # You  can also consider the gradient descent with momentum\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = gradient_descent(age_train, features_train)\n",
    "loss, pred = evaluate(w, b, age_val, features_val)\n",
    "print(\"Your validate score is:\", round(loss, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Test and Generate results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = test(w, b, features_test, 'gd.txt')\n",
    "print(\"Test results has saved to gd.txt\")\n",
    "print(prediction[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Stochastic Gradient descent\n",
    "Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning.\n",
    "```\n",
    "Arguments:\n",
    "    age          -- numpy array, label, (n, )\n",
    "    feature      -- numpy array, features, (n, 2048)\n",
    "Return:\n",
    "    weights      -- numpy array, (2048, )\n",
    "    bias         -- numpy array, (1, )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(age, feature):\n",
    "    # check the inputs\n",
    "    assert len(age) == len(feature)\n",
    "    \n",
    "    # Set the random seed\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Init weights and bias\n",
    "    weights = np.random.rand(2048, 1)\n",
    "    bias = np.random.rand(1, 1)\n",
    "\n",
    "    # Learning rate\n",
    "    lr = 10e-5\n",
    "\n",
    "    # Batch size\n",
    "    batch_size = 16\n",
    " \n",
    "    # Number of mini-batches\n",
    "    t = len(age) // batch_size\n",
    "\n",
    "    for e in range(epoch_sgd):\n",
    "        # Shuffle training data\n",
    "        n = np.random.permutation(len(feature))  \n",
    "        \n",
    "        for m in range(t):\n",
    "            # Providing mini batch with fixed batch size of 16\n",
    "            batch_feature = feature[n[m * batch_size : (m+1) * batch_size]]\n",
    "            batch_age = age[n[m * batch_size : (m+1) * batch_size]]\n",
    "            \n",
    "            ##########################################################################\n",
    "            # TODO: YOUR CODE HERE\n",
    "            ########################################################################## \n",
    "            \n",
    "            l_w = np.zeros((2048, 1), dtype=float)\n",
    "            l_b = 0\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                tmp = batch_feature[i, :].reshape(2048, 1)\n",
    "                l_w += (np.dot(batch_feature[i, :], weights) + bias - float(batch_age[i])) * tmp\n",
    "            l_w /= batch_size\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                l_b += np.dot(batch_feature[i, :], weights) + bias - float(batch_age[i])\n",
    "            l_b /= batch_size\n",
    "            \n",
    "            weights -= lr * l_w\n",
    "            bias -= lr * l_b\n",
    "   \n",
    "                \n",
    "            if momentum:\n",
    "                pass # You can also consider the gradient descent with momentum\n",
    "        \n",
    "        print('=> epoch:', e + 1, '  Loss:', round(loss,4))\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w, b = stochastic_gradient_descent(age_train, features_train)\n",
    "loss, pred = evaluate(w, b, age_val, features_val)\n",
    "print(\"Your validate score is:\", round(loss, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Generate results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = test(w, b, features_test, 'sgd.txt')\n",
    "print(\"Test results has saved to sgd.txt\")\n",
    "print(prediction[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}